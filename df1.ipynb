{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Explorant el mon de les cares amb Intel·ligència Artificial\n",
    "\n",
    "**DevFest Menorca 2024**, 24 octubre 2024\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bmalcover/devFest24/blob/main/df1.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## Sobre mi\n",
    "\n",
    "*   Soy **Biel Moyà** (gabriel.moya *at* uib.es)y trabajo como profesor e investigador en la UIB.\n",
    "*   Doy clases de Inteligencia Artificial y de Aprendizaje Automàtico en el grado de informática. También de introducción a la programación en el grado de Matemáticas.\n",
    "*   Investigo en técnicas de IA  normalmente aplicadas a imágenes médicas: radiografías, resonancias magnéticas, heridas, pies con úlceras, uñas enfermas ... este tipo de problemas tienen ciertas similitudes.\n",
    "* Tambien intento ayudar a entender como funcionan las redes neuronales profundas, ya que de momento para nosotros son similares a cajas oscuras que nos resultan imposibles de interpretar.\n",
    "* Me encanta el lenguaje Python. Y aunque no soy el mejor programador de la historia me entretiene (y me gusta) mucho hacerlo.\n",
    "\n",
    "## Sobre el taller de hoy\n",
    "\n",
    "* Os explicaré porque he elegido la temática de las caras, cuando existen otros infinitos problemas que podría haber propuesto.\n",
    "* Entenderemos que una imágen es una representación visual de alta dimensionalidad.\n",
    "* Aplicaremos diversas técnicas de IA a imágenes de caras para extraer información de su contenido y veremos que es posible hacerlo sin la necesidad de realizar código muy complicado.\n",
    "* Veremos como podemos reutilizar trabajo ya hecho y aplicar un cambio de estilo a una imagen.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Tl_dnhsDqF64"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Contexto\n",
    "## **El reconocimiento de caras en humanos**\n",
    "\n",
    "El cerebro del recién nacido está programado, desde su nacimiento, para buscar y detectar rostros. Algo que resulta importante para su supervivencia y desarrollo.\n",
    "\n",
    "En una investigación, desarrollada por psicólogos de la Universidad Emory en Atlanta, Estados Unidos, se analizó el córtex visual de bebés recién nacidos a través de imágenes de resonancia magnética funcional. A través del escáner pudieron demostrar que desde el sexto día tras el nacimiento, el cerebro de los bebés está programado con las tareas específicas de buscar y observar rostro\n",
    "[enlace](https://www.criarconsentidocomun.com/el-cerebro-del-recien-nacido-detecta-rostros/).\n",
    "\n",
    "## **Caras y su apariencia**\n",
    "\n",
    "![Cares](figuras/faces.png \"caras\")\n",
    "\n",
    "# Caras en las imágenes\n",
    "\n",
    "El reconocimiento de caras ha jugado un papel central en la historia de la IA, tanto en su evolución como en su aplicación. Esta tecnología ha sido clave en diversos momentos históricos por su impacto tanto en el ámbito técnico como en el social.\n",
    "\n",
    "\n",
    "1. **Inicio del estudio y desarrollo de patrones**\n",
    "\n",
    "El reconocimiento de caras fue uno de los primeros campos de aplicación de la **visión por computador**, una rama de la IA dedicada a que las máquinas puedan **ver y comprender imágenes y vídeos**. Ya en los años 60, se empezaron a estudiar métodos para reconocer patrones en imágenes, incluyendo caras humanas. Uno de los primeros intentos consistió en comparar medidas geométricas de las caras, como la distancia entre los ojos o el contorno facial, para clasificarlas.\n",
    "\n",
    "2. **Evolución gracias a los algoritmos de machine learning**\n",
    "\n",
    "A medida que los algoritmos de machine learning avanzaron, el reconocimiento facial se benefició enormemente.\n",
    "\n",
    "Hacia los años 90, se desarrollaron técnicas como el **análisis de componentes principales (PCA)** y el algoritmo conocido como [Eigenfaces](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html), que representaban a las caras como combinaciones lineales de una serie de \"componentes principales\". Éste fue un gran salto hacia la precisión, ya que permitió representar una cara de manera más robusta y matemática.\n",
    "\n",
    "\n",
    "El algoritmo Viola-Jones es una de las técnicas más conocidas y utilizadas para la **detección** (que no el reconocimiento) de caras en imágenes. Creado por Paul Viola y Michael Jones en 2001, este algoritmo fue uno de los primeros que permitió una detección de caras en tiempo real de forma efectiva. Aunque ya han surgido técnicas más avanzadas como las redes neuronales convolucionales, Viola-Jones sigue siendo importante gracias a su velocidad y eficiencia para la detección de caras frontales.\n",
    "\n",
    "![viola](figuras/viola_jones.png \"Viola Jones\")\n",
    "\n",
    "\n",
    "\n",
    "3. **Introducción de redes neuronales**\n",
    "\n",
    "El gran avance del reconocimiento de caras llegó con la aparición de las redes neuronales y el deep learning (profundo aprendizaje). Las redes neuronales convolucionales (CNNs) se mostraron particularmente efectivas para la tarea de reconocer caras, puesto que son excelentes para la detección de patrones en imágenes. Los sistemas modernos de reconocimiento facial, como los utilizados por empresas como Facebook (Meta), Amazon, Google o Apple, se basan en estas redes neuronales para aprender características faciales de forma automática, mejorando la eficiencia y precisión del reconocimiento en entornos realistas.\n",
    "\n",
    "![CNN](figuras/alexNet.png \"cnn\")\n",
    "\n",
    "\n",
    "\n",
    "## Aplicaciones prácticas\n",
    "\n",
    " Seguridad: El reconocimiento facial se ha utilizado ampliamente en sistemas de vigilancia, control de acceso y aplicaciones de identificación. Por ejemplo, muchos aeropuertos utilizan sistemas de reconocimiento facial para verificar la identidad de los pasajeros.\n",
    "\n",
    " Dispositivos móviles: Con la popularidad de smartphones como el iPhone con Face ID, el reconocimiento de caras se ha convertido en una de las formas más comunes de autenticación biométrica.\n",
    "\n",
    " Identificación en redes sociales: Plataformas como Facebook han utilizado el reconocimiento facial para ayudar a etiquetar a personas automáticamente en fotos.\n",
    "\n",
    "## Impulso a la investigación en ética y privacidad\n",
    "\n",
    "El reconocimiento de caras también ha generado un importante debate sobre la privacidad y derechos civiles. A medida que la tecnología se ha vuelto más precisa, sus aplicaciones en la vigilancia masiva han sido objeto de críticas por parte de diversas organizaciones de derechos humanos.\n",
    "\n",
    "Esto ha estimulado la creación de marcos legales y éticos en torno a la IA, dando lugar a discusiones sobre transparencia y control de datos personales.\n",
    "\n"
   ],
   "metadata": {
    "id": "9xWK-ocMD21W"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Que son las imágenes digitales?\n",
    "\n",
    "Las imágenes digitales son una representación bidimensional de una imagen en una estructura de datos (una matriz). Estan formadas por números que siguen una organización determinada."
   ],
   "metadata": {
    "id": "A5A_NOBcuu2X"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "img = cv2.imread(\"img/test.png\")\n",
    "\n",
    "plt.imshow(img);"
   ],
   "metadata": {
    "id": "IFa-bO_Uuuhv",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "outputId": "0ef11348-2a9b-4205-9e25-5f2cad9090f9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación inspeccionaremos e intentaremos entender como es una imágen digital:\n",
    "\n",
    "* Que tamaño tiene? Cuantos datos manejamos? Que tipo de datos usa?\n",
    "* Como puedo seleccionar trozos de la imagen?\n",
    "* Puedo modificar partes de la imagen de manera senzilla?\n"
   ],
   "metadata": {
    "id": "2QO7lNhdz4kw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: Explorar imagen"
   ],
   "metadata": {
    "id": "g9kwM2R4z3lE"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DeepFace\n",
    "\n",
    "\n",
    "DeepFace es un sistema de reconocimiento facial de aprendizaje profundo creado por un equipo de desarrollo en Facebook. Este define rostros humanos en imágenes digitales. Emplea una red neuronal de 9 capas con más de 120 millones de conexiones, y ha sido entrenado en 4 millones de imágenes subidas por los usuarios de Facebook. Se ha mencionado que el sistema tiene un 97% de aciertos, comparado con el 85% del sistema Next Generation Identification del FBI. [wikipedia](https://es.wikipedia.org/wiki/DeepFace).\n",
    "\n",
    "El repositorio de código de esta libreria es el [siguiente](https://github.com/serengil/deepface). En el podemos encontrar más información de sus diferentes funcionalidades.\n"
   ],
   "metadata": {
    "id": "tjv5XVQs16B3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#!pip install opencv-python deepface # tenemos que instalarla"
   ],
   "metadata": {
    "id": "cNbiIsXZQiHF"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Librerias que nos permiten realizar tareas sin tener que programarlas\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "apfSPeNn7mhA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Empezaremos con una tarea sencilla, encontrar una cara en una imagen y dibujar la región que ocupa.\n",
    "\n"
   ],
   "metadata": {
    "id": "uYMQuioXPAwT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "IMG_PATH = \"/content/img/facel.png\"\n",
    "detected_faces = DeepFace.extract_faces(img_path=IMG_PATH)\n",
    "\n",
    "face_data = detected_faces[0]['facial_area']\n",
    "#print(face_data)\n",
    "\n",
    "cv_img = cv2.imread(IMG_PATH)\n",
    "\n",
    "inicio = (face_data[\"x\"], face_data[\"y\"])\n",
    "final = (face_data[\"x\"] + face_data[\"w\"], face_data[\"y\"] + face_data[\"h\"])\n",
    "color = (0, 255, 0) \n",
    "grosor = 4 # pixeles\n",
    "\n",
    "face_with_box = cv2.rectangle(cv_img, inicio, final, color, grosor)\n",
    "\n",
    "# Mostramos la figura con la caja alrededor de la cara\n",
    "plt.imshow(face_with_box);"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "collapsed": true,
    "id": "MTveODVp86eY",
    "outputId": "97326418-7870-4664-80ff-f3c6e8f08075"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "DeepFace  nos permite realizar un montón de tareas:\n",
    "\n",
    "### Análisis de los atributos de una persona\n",
    "\n",
    "A partir de la detección de una cara, se pueden estimar (que no conocer) características de una persona por ejemplo:\n",
    "- La edad\n",
    "- El género\n",
    "- La raza\n",
    "- Su emoción predominante\n",
    "\n",
    "Veamos como podemos hacerlo:"
   ],
   "metadata": {
    "id": "ekBBdWU4K5hp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "objs = DeepFace.analyze(\n",
    "  img_path = IMG_PATH,\n",
    "  actions = ['age', 'gender', 'race', 'emotion'],\n",
    ");\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Que podemos saber? {list(objs[0].keys())}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0rR-RZ6K6oA",
    "outputId": "473448f4-94f6-4c05-8275-796540fccabc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comprobar el envejecimiento de una persona\n",
    "\n",
    "Vamos a ver que predicciones realiza este método respecto a la edad de una persona quue conocemos bien."
   ],
   "metadata": {
    "id": "H2oRnaFUOGWt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "leo_joven = \"/content/img/Leonardo_DiCaprio.jpeg\"\n",
    "leo_menos_joven = \"/content/img/Leonardo_DiCaprio_2010.jpg\"\n",
    "leo_2024 = \"/content/img/Leonardo_DiCaprio_2024.jpeg\"\n",
    "\n",
    "objs = DeepFace.analyze(\n",
    "  img_path = leo_joven,\n",
    "  actions = ['age'],\n",
    ")\n",
    "\n",
    "objs1 = DeepFace.analyze(\n",
    "  img_path = leo_menos_joven,\n",
    "  actions = ['age'],\n",
    ")\n",
    "\n",
    "objs2 = DeepFace.analyze(\n",
    "  img_path = leo_2024,\n",
    "  actions = ['age'],\n",
    ")\n",
    "\n",
    "\n",
    "lj   = cv2.imread(leo_joven)\n",
    "lj = cv2.cvtColor(lj, cv2.COLOR_BGR2RGB)\n",
    "lj_m = cv2.imread(leo_menos_joven)\n",
    "lj_m = cv2.cvtColor(lj_m, cv2.COLOR_BGR2RGB)\n",
    "lj_2024 = cv2.imread(leo_2024)\n",
    "lj_2024 = cv2.cvtColor(lj_2024, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(lj)\n",
    "ax[0].set_title(f\"Leo joven {objs[0]['age']}\")\n",
    "xax = ax[0].axes.get_xaxis()\n",
    "xax = xax.set_visible(False)\n",
    "yax = ax[0].axes.get_yaxis()\n",
    "yax = yax.set_visible(False)\n",
    "\n",
    "ax[1].imshow(lj_m)\n",
    "ax[1].set_title(f\"Leo menos joven {objs1[0]['age']}\")\n",
    "xax = ax[1].axes.get_xaxis()\n",
    "xax = xax.set_visible(False)\n",
    "yax = ax[1].axes.get_yaxis()\n",
    "yax = yax.set_visible(False)\n",
    "\n",
    "ax[2].imshow(lj_2024)\n",
    "ax[2].set_title(f\"Menos menos joven {objs2[0]['age']}\")\n",
    "xax = ax[2].axes.get_xaxis()\n",
    "xax = xax.set_visible(False)\n",
    "yax = ax[2].axes.get_yaxis()\n",
    "yax = yax.set_visible(False)\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "2KJgh0K2Xa9j",
    "outputId": "aa48664e-8eec-4caf-9040-c6e3fcc9ccfe"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quien se parece a Leonardo?\n",
    "\n",
    "Dicen que Leonardo di Caprio se parece a Jack Nicholson, será verdad?\n",
    "\n",
    "La libreria DeepFace permite que sus funciones internamente usen diferentes métodos. Esto se llama backend y tenemos los siguientes: `backends = ['opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface', 'mediapipe']` entre otros."
   ],
   "metadata": {
    "id": "G0NIKDyHkH55"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mi_imagen = \"/content/img/jo_mateix.jpg\"\n",
    "leo_verano = \"/content/img/Leonardo_DiCaprio_verano.png\"\n",
    "jack = \"/content/img/jack_nicholson.png\"\n",
    "res = DeepFace.verify(leo_verano, jack) #, detector_backend=\"retinaface\")\n"
   ],
   "metadata": {
    "id": "BEBYNglIdkii"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Que información tenemos en la variable `res`?"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "res"
  },
  {
   "cell_type": "code",
   "source": [
    "#Hacemos una función para que el código quede más digno\n",
    "def dibujar_cara(path, face_data, aX):\n",
    "\n",
    "  inicio = (face_data[\"x\"], face_data[\"y\"])\n",
    "  final = (face_data[\"x\"] + face_data[\"w\"], face_data[\"y\"] + face_data[\"h\"])\n",
    "  color = (0, 255, 0) \n",
    "  grosor = 4 # pixeles\n",
    "  face_with_box = cv2.rectangle(img, inicio, final, color, grosor)\n",
    "\n",
    "  return face_with_box\n",
    "\n",
    "cara1 = res[\"facial_areas\"][\"img1\"]\n",
    "cara2 = res[\"facial_areas\"][\"img2\"]\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(dibujar_cara(leo_verano, cara1))\n",
    "ax[0].set_title(f\"Leo\")\n",
    "xax = ax[0].axes.get_xaxis()\n",
    "xax = xax.set_visible(False)\n",
    "yax = ax[0].axes.get_yaxis()\n",
    "yax = yax.set_visible(False)\n",
    "\n",
    "ax[1].imshow(dibujar_cara(jack, cara2))\n",
    "ax[1].set_title(f\"Jack\")\n",
    "xax = ax[1].axes.get_xaxis()\n",
    "xax = xax.set_visible(False)\n",
    "yax = ax[1].axes.get_yaxis()\n",
    "yax = yax.set_visible(False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "MO02pToHwIjY",
    "outputId": "6285d9d1-ecac-4ad0-a06b-40804b14d558"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Encontrando los puntos claves de las caras\n",
    "\n",
    "Los puntos de referencia faciales de Dlib se refieren a puntos específicos en un rostro que se identifican para ayudar con las tareas de análisis facial. Estos puntos, a menudo llamados puntos de referencia, generalmente se colocan alrededor de rasgos faciales clave, como los ojos, la nariz, la boca y la línea de la mandíbula.\n",
    "\n",
    "Dlib proporciona un modelo entrenado previamente que puede detectar 68 puntos de referencia faciales, que se utilizan comúnmente para tareas como la alineación de rostros, el reconocimiento de emociones o el intercambio de rostros. \n",
    "\n",
    "[Enlace a la base de datos original](https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/)\n",
    "\n",
    "\n",
    "![Cares](figuras/facial_landmarks.jpg \"caras\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#!pip install dlib"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import dlib\n",
    "import numpy as np\n",
    "PREDICTOR_PATH = \"shape_predictor_68_face_landmarks.dat\"\n",
    "predictor = dlib.shape_predictor(PREDICTOR_PATH)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "\n",
    "class TooManyFaces(Exception):\n",
    "    pass\n",
    "\n",
    "class NoFaces(Exception):\n",
    "    pass\n",
    "\n",
    "def get_landmarks(im):\n",
    "    rects = detector(im, 1)\n",
    "\n",
    "    if len(rects) > 1:\n",
    "        raise TooManyFaces\n",
    "    if len(rects) == 0:\n",
    "        raise NoFaces\n",
    "\n",
    "    return np.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])\n",
    "\n",
    "def annotate_landmarks(im, landmarks):\n",
    "    im = im.copy()\n",
    "    for idx, point in enumerate(landmarks):\n",
    "        pos = (point[0, 0], point[0, 1])\n",
    "        cv2.putText(im, str(idx), pos,\n",
    "                    fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n",
    "                    fontScale=0.4,\n",
    "                    color=(0, 0, 255))\n",
    "        cv2.circle(im, pos, 3, color=(0, 255, 255))\n",
    "    return im\n",
    "\n",
    "\n",
    "detected_faces = DeepFace.extract_faces(img_path=IMG_PATH)\n",
    "\n",
    "\n",
    "\n",
    "image = cv2.imread(\"/content/img/facel.jpg\")\n",
    "\n",
    "landmarks = get_landmarks(image)\n",
    "image_with_landmarks = annotate_landmarks(image, landmarks)\n",
    "image_with_landmarks = cv2.cvtColor(image_with_landmarks, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "plt.imshow(image_with_landmarks);\n"
   ]
  }
 ]
}
